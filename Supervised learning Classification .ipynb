{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning: classification\n",
    "\n",
    "In this exercise we will work with three types of classification models. To do so, we will make use of functions from the SKLEARN library. This library contains a lot of machine learning algorithms that are used in practise, so it is very valuable to familiarize yourself with the SKLEARN library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "#import csv\n",
    "\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/Daphne310/TrainingCases/master/iris-data-training.csv')\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation \n",
    "\n",
    "We just loaded the iris dataset. It contains data of three types of irisses. The features are measures of the lenght and width of the sepal and petal. Let's first split the data in features and targets. The iris dataset is a well-defined set for educational purposes, but in practise you will have to define the features and targets before this step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iris[['sepal_length_cm', 'sepal_width_cm',\n",
    "                             'petal_length_cm', 'petal_width_cm']].values\n",
    "target = iris['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding of the data, we will print the first example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The features of the first example are: ', data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The label of the first example is: ', target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is to create a trainset and a testset and to scale the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, \n",
    "                                                  test_size=1/3, random_state=123)\n",
    "\n",
    "print(X_train.shape)\n",
    "iris.loc[(iris['sepal_length_cm'].isnull()) |\n",
    "              (iris['sepal_width_cm'].isnull()) |\n",
    "              (iris['petal_length_cm'].isnull()) |\n",
    "              (iris['petal_width_cm'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the classifier, we follow a few simple steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Choose the type of classifier \n",
    "DTalgorithm = tree.DecisionTreeClassifier()\n",
    "\n",
    "#STEP 2: Fit the model to the train data\n",
    "DTalgorithm = DTalgorithm.fit(X_train, y_train)\n",
    "\n",
    "#STEP 3: Predict the labels of the test data \n",
    "Pred_DT = DTalgorithm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just made a predictive algorithm!\n",
    "\n",
    "The variable 'Pred_DT' holds the predictions made by the algorithm. Now let's take a look at your performance using classification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_CA = accuracy_score(y_test, Pred_DT) \n",
    "\n",
    "print('The Decision Tree Classifier reached a Classification Accuracy of: ', DT_CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you get a good score? \n",
    "\n",
    "We are not done yet, though. We might be able to reach a better performance by tuning the parameters. The Decision Tree Classifier has lots of parameters, but the most important ones are: \n",
    "\n",
    "- Criterion (measure of impurity: the way the algorithm decides where to split the data, optional parameters are 'gini' and 'entropy'). \n",
    "- Splitter (strategy to choose the split, optional parameters are 'best' and 'random').\n",
    "- Maximal depth (how deep the tree should go). \n",
    "\n",
    "Go back to the code in STEP 1 and play around with the parameters. \n",
    "\n",
    "For example: DTalgorithm = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=7) \n",
    "\n",
    "Check the impact this has on the classification accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the classifier, we follow a few simple steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Choose the type of classifier \n",
    "KNNalgorithm = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "#STEP 2: Fit the model to the train data\n",
    "KNNalgorithm = KNNalgorithm.fit(X_train, y_train)\n",
    "\n",
    "#STEP 3: Predict the labels of the test data \n",
    "Pred_KNN = KNNalgorithm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You did it again!\n",
    "\n",
    "The variable 'Pred_KNN' holds the predictions made by the algorithm. Now let's take a look at your performance using classification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_CA = accuracy_score(y_test, Pred_KNN) \n",
    "\n",
    "print('The K-Nearest Neighbors Classifier reached a Classification Accuracy of: ', KNN_CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you get a good score? \n",
    "\n",
    "By know you probably know the drill. Tuning the parameters might help to reach a better performance. For K-Nearest Neighbors the most important parameter is the number of neighbors taken into account. The code now says 'n_neighbors = 3'. Play around with the number to see if the model can be improved. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "To construct the classifier, we follow a few simple steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Choose the type of classifier \n",
    "LRalgorithm = LogisticRegression() \n",
    "\n",
    "#STEP 2: Fit the model to the train data\n",
    "LRalgorithm = LRalgorithm.fit(X_train, y_train)\n",
    "\n",
    "#STEP 3: Predict the labels of the test data \n",
    "Pred_LR = LRalgorithm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheers! Another model nicely constructed! Let's check the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CA = accuracy_score(y_test, Pred_LR) \n",
    "\n",
    "print('The Logistic Regression Classifier reached a Classification Accuracy of: ', LR_CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic Regression there are lots of parameters. The most important ones to play around with are: \n",
    "\n",
    "- penalty, optional values: 'l1' or 'l2'\n",
    "- solver, optional values: 'newton-cg', 'lbfgs', 'liblinear', 'sag' and 'saga'\n",
    "\n",
    "Example: LRalgorithm = LogisticRegression(penalty='l2', solver='liblinear') \n",
    "\n",
    "Go ahead, try them out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "What model eventually reached the highest classification accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Decision Tree Classifier reached a Classification Accuracy of: ', DT_CA)\n",
    "print('The K-Nearest Neighbors Classifier reached a Classification Accuracy of: ', KNN_CA)\n",
    "print('The Logistic Regression Classifier reached a Classification Accuracy of: ', LR_CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we automate part of the analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of Algorithms\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "#url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "#dataframe = pandas.read_csv(url, names=names)\n",
    "#array = dataframe.values\n",
    "#X = array[:,0:8]\n",
    "#Y = array[:,8]\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
